# -*- coding: utf-8 -*-
"""NLP Translate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nYprbgx6fzT6orWlwHQlrmzU2b7k9ad0

## Import Semua Packages/Library yang Digunakan
"""

import re
import string
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
!pip install PySastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import pandas as pd
import nltk
nltk.download('punkt')
nltk.download('stopwords')
import requests
import csv
from io import StringIO
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from nltk.stem import SnowballStemmer
from tqdm import tqdm
import nltk
nltk.download('punkt')
nltk.download('stopwords')
import requests
import csv
from io import StringIO
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from nltk.stem import SnowballStemmer
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import pandas as pd
import nltk
nltk.download('punkt')
nltk.download('stopwords')
import requests
import csv
from io import StringIO
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from nltk.stem import SnowballStemmer
from tqdm import tqdm
import nltk
nltk.download('punkt_tab')
nltk.download('stopwords')
import requests
import csv
from io import StringIO
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from nltk.stem import SnowballStemmer
import numpy as np
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from collections import defaultdict
import os
import kagglehub
from nltk.tokenize import sent_tokenize
import time
import tensorflow as tf
!pip install transformers
from transformers import TFT5ForConditionalGeneration, T5Tokenizer
!pip install tensorflow
!pip install nltk
!pip install sentencepiece
!pip install scikit-learn

!pip install transformers[torch] faiss-cpu sentencepiece datasets

!pip install firebase-admin

from tensorflow.keras.layers import Input, Embedding, LSTM, Dense

!pip install faiss-cpu

import faiss

# Load the dataset
data = pd.read_csv('/content/translated_train.csv')

# Preprocess function
def preprocess_text(text):
    # Check if the text is a string before processing
    if isinstance(text, str):
        # Convert to lowercase
        text = text.lower()
        # Remove punctuation
        text = text.translate(str.maketrans('', '', string.punctuation))
        # Remove numbers
        text = re.sub(r'\d+', '', text)
        # Tokenize and remove stopwords
        stop_words = set(stopwords.words('indonesian'))
        words = word_tokenize(text)
        text = ' '.join([word for word in words if word not in stop_words])
        return text
    else:
        # Handle non-string values, e.g., by returning an empty string or NaN
        return ''  # or pd.NA

# Apply preprocessing to the context and response columns
data['processed_context'] = data['translated_context'].apply(preprocess_text)
data['processed_response'] = data['translated_response'].apply(preprocess_text)

# Show the preprocessed data
data[['translated_context', 'processed_context', 'translated_response', 'processed_response']].head()

data['translated_response'].fillna("Sorry, I have no answer for this.", inplace=True)
data = data.drop_duplicates(subset=['processed_context', 'processed_response']).reset_index(drop=True)

df_terjemahan.head()

from google.colab import files
uploaded = files.upload()

import firebase_admin
from firebase_admin import credentials, firestore

# Inisialisasi Firebase
cred = credentials.Certificate("mindfulness-a24a6-firebase-adminsdk-fbsvc-65a1ea1d0c.json") # cloud firestore buat simpan riwayat chat
firebase_admin.initialize_app(cred)

# Buat koneksi ke database Firestore
db = firestore.client()

# Cek jumlah missing value per kolom
print(df_terjemahan.isna().sum())

# Hapus baris duplikat berdasarkan Context & Response (kalau ada), lalu buat salinan aman
df_terjemahan = df_terjemahan.drop_duplicates(subset=['Context', 'Response']).copy()

# Isi NaN pada kolom 'Response' jika ada
if 'Response' in df_terjemahan.columns:
    df_terjemahan['Response'] = df_terjemahan['Response'].fillna(
        "Maaf, aku tidak punya jawaban untuk ini."
    )

# Isi NaN pada kolom 'translated_response' jika ada
if 'translated_response' in df_terjemahan.columns:
    df_terjemahan['translated_response'] = df_terjemahan['translated_response'].fillna(
        "Sorry, I have no answer for this."
    )

print("Jumlah sampel:", len(df_terjemahan))

from transformers import AutoTokenizer, TFAutoModel

# Model untuk Retrieval (Encoder)
retriever_model_name = "cahya/distilbert-base-indonesian"

# Gunakan AutoTokenizer
retriever_tokenizer = AutoTokenizer.from_pretrained(retriever_model_name)
retriever_model = TFAutoModel.from_pretrained(retriever_model_name, from_pt=True)

print(f"Model Retriever '{retriever_model_name}' dan Tokenizer berhasil dimuat dalam format TensorFlow.")

# jadi df
df = data.copy()
df = df.drop_duplicates(subset=['processed_context', 'processed_response']).reset_index(drop=True)
df['processed_response'] = df['processed_response'].fillna("maaf aku tidak punya jawaban untuk ini")

from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load tokenizer dan model IndoBERT
tokenizer = AutoTokenizer.from_pretrained("cahya/distilbert-base-indonesian")
model = AutoModel.from_pretrained("cahya/distilbert-base-indonesian")

# Define device here, outside of functions
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Move the model to the device
model.to(device)


def get_batch_embeddings(texts, batch_size=16):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]

        # Tokenisasi dan pindah ke device
        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128)

        # Pastikan semua input berada di device yang sama
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            # Model is already on the device
            outputs = model(**inputs)
        last_hidden = outputs.last_hidden_state
        attention_mask = inputs['attention_mask']

        # Mean Pooling
        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
        summed = torch.sum(last_hidden * mask_expanded, 1)
        counts = torch.clamp(mask_expanded.sum(1), min=1e-9)
        mean_pooled = summed / counts

        embeddings.append(mean_pooled.cpu().numpy())

    return np.vstack(embeddings)


def get_query_embedding(query):
    preprocessed = preprocess_text_indonesian(query)  # Fungsi pra-pemrosesan kamu
    inputs = tokenizer(preprocessed, return_tensors='pt', truncation=True, padding=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        # Model is already on the device
        outputs = model(**inputs)

    last_hidden = outputs.last_hidden_state
    attention_mask = inputs['attention_mask']
    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
    summed = torch.sum(last_hidden * mask_expanded, 1)
    counts = torch.clamp(mask_expanded.sum(1), min=1e-9)
    mean_pooled = summed / counts

    # Reshape to 2D: This is the crucial fix
    return mean_pooled.cpu().numpy().reshape(1, -1)

def get_semantic_chatbot_response_with_fallback(user_query, df, context_embeddings, similarity_threshold=0.3):
    preprocessed_query = preprocess_text_indonesian(user_query)
    print(f"Query pengguna setelah pra-pemrosesan: '{preprocessed_query}'")

    if not preprocessed_query.strip():
        return "Maaf, pertanyaan Anda kosong atau tidak dapat dipahami."

    embedding = get_query_embedding(preprocessed_query)
    similarity_scores = cosine_similarity(embedding, context_embeddings)[0]
    max_score = np.max(similarity_scores)

    print(f"Skor kemiripan tertinggi: {max_score:.4f}")

    if max_score < similarity_threshold:
        print(f"Skor kemiripan ({max_score:.4f}) < threshold ({similarity_threshold})")
        return "Maaf, saya belum memahami pertanyaan Anda dengan baik. Bisa dijelaskan dengan cara lain?"

    best_idx = np.argmax(similarity_scores)
    response_col = 'translated_response'

    try:
        response_text = df.iloc[best_idx][response_col]
        print(f"Indeks respons terbaik: {best_idx}")
        print(f"Respons yang ditemukan: {response_text}")
        if not isinstance(response_text, str) or not response_text.strip():
            return "Maaf, saya tidak menemukan jawaban yang sesuai."
        return response_text
    except KeyError:
        return f"Error: Kolom '{response_col}' tidak ditemukan."
    except Exception as e:
        print(f"Kesalahan saat mengambil respons: {e}")
        return "Maaf, terjadi kesalahan teknis."

def encode_text(text):
    inputs = retriever_tokenizer(
        text, return_tensors='tf', padding=True, truncation=True, max_length=128
    )
    outputs = retriever_model(**inputs)
    embeddings = outputs.last_hidden_state
    embeddings = tf.reduce_mean(embeddings, axis=1).numpy()
    return embeddings.squeeze()

corpus_embeddings = np.array([encode_text(t) for t in df['processed_context']])
index = faiss.IndexFlatL2(corpus_embeddings.shape[1])
index.add(corpus_embeddings)

def retrieve(query, k=1):
    query_embedding = encode_text(preprocess_text(query)).reshape(1, -1)
    _, I = index.search(query_embedding, k)
    return df.iloc[I[0]]

history = []  # menyimpan chat sebelumnya
banned_words = {'bodoh', 'sialan', 'kasar', 'anjing', 'babi', 'brengsek', 'bangsat', 'bajingan',
    'goblok', 'tolol', 'idiot', 'bodoh', 'dungu', 'kafir', 'munafik', 'syirik', 'Pokkai',
    'fuck', 'fucking', 'shit', 'shitty', 'bitch', 'fkn', 'sht', 'b1tch', 'b!tch', 'f4ck'
    'bastard', 'asshole', 'damn', 'damnit', 'crap',
    'dick', 'douche', 'piss', 'pissed', 'bollocks',
    'bugger', 'bloody', 'arse', 'arsehole', 'retard', 'moron', 'crackhead', 'degenerate',
    'scumbag', 'tool', 'twat',
    'setan', 'iblis', 'sialan', 'kampret', 'pecun',}

def is_banned(text):
    return any(word in text.lower() for word in banned_words)

def mindfulness_chat():
    print("Mindfulness siap mendengarkan (ketik 'exit' untuk berhenti).")

    while True:
        user_input = input("Kamu: ")

        if user_input.lower() in ['exit', 'quit']:
            print("Mindfulness: Sampai jumpa lagi, tetap jaga dirimu ya.")
            break

        if is_banned(user_input):
            print("Mindfulness: Maaf, aku tidak bisa menanggapi hal tersebut.")
            continue

        # Gabungkan konteks percakapan sebelumnya (terakhir 3 percakapan)
        recent_context = " ".join(h['user'] for h in history[-3:])
        combined_query = recent_context + " " + user_input if recent_context else user_input

        # Ambil respon dari retriever
        retrieved_docs = retrieve(combined_query, k=1)
        jawaban = retrieved_docs['processed_response'].values[0]

        # Tampilkan respon
        print(f"Mindfulness: {jawaban}")

        # Simpan ke history lokal
        history.append({"user": user_input, "bot": jawaban})

        # Simpan ke Firestore
        simpan_riwayat_ke_firestore(user_input, jawaban)

def simpan_riwayat_ke_firestore(user_input, bot_response):
    doc = {
        'user': user_input,
        'bot': bot_response,
        'timestamp': firestore.SERVER_TIMESTAMP
    }
    db.collection('chat_history').add(doc)

dist_list = []
for q in df['processed_context'].sample(50):
    q_emb = encode_text(q).reshape(1, -1)
    d, _ = index.search(q_emb, k=1)
    dist_list.append(d[0][0])

plt.hist(dist_list, bins=20)
plt.title("Distribusi Nilai Similarity (Distance) di FAISS")
plt.xlabel("Jarak")
plt.ylabel("Jumlah")
plt.show()

def get_semantic_chatbot_response_with_fallback(query, df, index, similarity_threshold=1.0):
    query_emb = encode_text(preprocess_text(query)).reshape(1, -1)
    distances, indices = index.search(query_emb, k=1)

    closest_distance = distances[0][0]
    closest_idx = indices[0][0]

    if closest_distance <= similarity_threshold:
        jawaban = df.iloc[closest_idx]['translated_response']
    else:
        jawaban = "Maaf, aku kurang memahami maksudmu. Bisa kamu jelaskan lebih lanjut?"

    print(f"Similarity distance: {closest_distance:.4f}")
    return jawaban

print("\nUji Coba Mindfulness\n")

queries = [
    "Saya merasa sangat sedih.",
    "Butuh bantuan untuk mengatasi kecemasan.",
    "Bagaimana seseorang memulai proses konseling?"
]

# treshold
SIMILARITY_THRESHOLD = 50.0

for i, query in enumerate(queries, start=1):
    print(f"Query Pengguna {i}: {query}")
    response = get_semantic_chatbot_response_with_fallback(
        query, df, index, similarity_threshold=SIMILARITY_THRESHOLD
    )
    print(f"Mindfulness {i}: {response}\n")

print("Uji Coba Mindfulness selesai.")

"""Simpan Model"""

faiss.write_index(index, 'mindfulness_index.faiss') # faiss

np.save('context_embeddings.npy', corpus_embeddings) # embedding

retriever_tokenizer.save_pretrained('mindfulness_tokenizer/') # Tokenizer Indobert